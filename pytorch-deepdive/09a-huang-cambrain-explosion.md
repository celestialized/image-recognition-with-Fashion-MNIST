# TPU's and the "Cambridge Explosion at NVIDIA
Cloud TPUs were designed to be clustered in datacenters, with 64 stacked processors dubbed “TPU pods” capable of 11.5 petaflops, according to Google CEO Sundar Pichai. The cloud-based Tensor processors are aimed at computer-intensive training of machine learning models as well as real-time tasks like making inferences about images.

Along with TensorFlow, Huang said Nvidia’s Volta GPU would be optimized for a range of machine-learning frameworks, including Caffe2 and Microsoft Cognitive Toolkit.

Nvidia is meanwhile releasing as open source technology its version of a “dedicated, inferencing TPU” called the Deep Learning Accelerator that has been designed into its Xavier chip for AI-based autonomous vehicles.

In parallel with those efforts, Google has been using its TPUs for the inference stage of a deep neural network since 2015. TPUs are credited with helping to bolster the effectiveness of various AI workloads, including language translation and image recognition programs, the company said.

The combination of processing power, cloud access and machine-learning training models are combining to fuel Huang’s projected “Cambrian explosion” of AI technology: “Deep learning is a strategic imperative for every major tech company,” he observed. “It increasingly permeates every aspect of work from infrastructure, to tools, to how products are made.”

# Tegra Xavier
Tegra Xavier is a 64-bit ARM high-performance system on a chip for autonomous machines designed by Nvidia and introduced in 2018. Xavier is incorporated into a number of Nvidia's computers including the Jetson Xavier, Drive Xavier, and the Drive Pegasus.

Jetson Xavier is packed with six processors which can run 30 trillion operations per second
"Essentially, a $10,000 workstation, 1,000 watts of performance, now fits in my hand for 30 watts." A development kit will start shipping in August (already did) from $1,299.

20 times more powerful than Nvidia's previous line of AI chips

## Other terms
Random forests
### NNP
Neural Network Processor 
### IPU
intelligence processing unit 
### TPU's
Tensor processing unit
### VPU
vision processing unit 
### ANN's
Atrtificial

### RNN's

sequence recognition
Wonder how astronomers found the autocorrective coding built into space monitoring?

### CNN's
spatial

A unique subset of accelerators are also called coprocessors.
* distinction lies in how an accelerator interacts with the host processor
  * co-processor is typically connected to the internals of the host processor
  * general accerator is seen as an independent i/o device programmed through an interface.

PXIE and sunmicro...

## Drive Pegasus
Label it correctly ONCE and cars get the benifit FOREVER. We want to be able to trace it- AND repeat it.

## Jetson nano
The new Jetson Nano has a tiny fraction of this power. Its GPU, where most of the AI processing takes place, has 128 Cuda processor cores for a total capacity of 472 gigaflops. 